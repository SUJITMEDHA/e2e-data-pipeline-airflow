# End-to-End Data Pipeline using Airflow

## Overview
This project demonstrates a production-style ETL pipeline using Apache Airflow.

## Tech Stack
- Python
- Apache Airflow
- Pandas
- REST APIs

## Pipeline Steps
1. Extract data from a public API
2. Transform and clean the data
3. Load processed data into storage (CSV)

## Key Learnings
- DAG orchestration
- Modular ETL design
- Data transformation best practices
